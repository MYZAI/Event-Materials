{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Architecture & Distilling CLIP for LLMs\n",
    "\n",
    "Welcome to the second session notebook of the **Math & AI Institute AI Talks** series.\n",
    "\n",
    "**Author:** DoÄŸukan Uraz Tuna (@dtunai)\n",
    "\n",
    "**Event Time:** 12 June, 2024 20:00 GMT+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U torch torch-vision datasets matplotlib numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Embedding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, width, max_seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, width)\n",
    "\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(width):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = np.sin(pos / (10000 ** (i / width)))\n",
    "                else:\n",
    "                    pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / width)))\n",
    "\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.pe\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, width, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.query = nn.Linear(width, head_size)\n",
    "        self.key = nn.Linear(width, head_size)\n",
    "        self.value = nn.Linear(width, head_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        attention = Q @ K.transpose(-2, -1)\n",
    "\n",
    "        attention = attention / (self.head_size**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        attention = attention @ V\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, width, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = width // n_heads\n",
    "\n",
    "        self.W_o = nn.Linear(width, width)\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(width, self.head_size) for _ in range(n_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = torch.cat([head(x, mask=mask) for head in self.heads], dim=-1)\n",
    "\n",
    "        out = self.W_o(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, width, n_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(width)\n",
    "\n",
    "        self.mha = MultiHeadAttention(width, n_heads)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(width)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.width, self.width * r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.width * r_mlp, self.width),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.mha(self.ln1(x), mask=mask)\n",
    "\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, encode=True, mask=None, max_seq_length=32):\n",
    "    if encode:\n",
    "        out = chr(2) + text + chr(3)\n",
    "        out = out + \"\".join([chr(0) for _ in range(max_seq_length - len(out))])\n",
    "        out = torch.IntTensor(list(out.encode(\"utf-8\")))\n",
    "        mask = torch.ones(len(out.nonzero()))\n",
    "        mask = torch.cat((mask, torch.zeros(max_seq_length - len(mask)))).type(\n",
    "            torch.IntTensor\n",
    "        )\n",
    "    else:\n",
    "        out = [chr(x) for x in text[1 : len(mask.nonzero()) - 1]]\n",
    "        out = \"\".join(out)\n",
    "        mask = None\n",
    "\n",
    "    return out, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Encoder's Text Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, width, max_seq_length, n_heads, n_layers, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, width)\n",
    "\n",
    "        self.positional_embedding = PositionalEmbedding(width, max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [TransformerEncoder(width, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "    def forward(self, text, mask=None):\n",
    "        x = self.encoder_embedding(text)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x, mask=mask)\n",
    "\n",
    "        x = x[torch.arange(text.shape[0]), torch.sub(torch.sum(mask[:, 0], dim=1), 1)]\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Encoder's Image ViT Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, width, img_size, patch_size, n_channels, n_layers, n_heads, emb_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert (\n",
    "            img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0\n",
    "        ), \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "        assert width % n_heads == 0, \"width must be divisible by n_heads\"\n",
    "\n",
    "        self.n_patches = (img_size[0] * img_size[1]) // (patch_size[0] * patch_size[1])\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "        self.linear_project = nn.Conv2d(\n",
    "            n_channels, width, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, width))\n",
    "        self.positional_embedding = PositionalEmbedding(width, self.max_seq_length)\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [TransformerEncoder(width, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_project(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = torch.cat((self.cls_token.expand(x.size()[0], -1, -1), x), dim=1)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        vit_width,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        n_channels,\n",
    "        vit_layers,\n",
    "        vit_heads,\n",
    "        vocab_size,\n",
    "        text_width,\n",
    "        max_seq_length,\n",
    "        text_heads,\n",
    "        text_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = ImageEncoder(\n",
    "            vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, emb_dim\n",
    "        )\n",
    "\n",
    "        self.text_encoder = TextEncoder(\n",
    "            vocab_size, text_width, max_seq_length, text_heads, text_layers, emb_dim\n",
    "        )\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, image, text, mask=None):\n",
    "        I_e = self.image_encoder(image)\n",
    "        T_e = self.text_encoder(text, mask=mask)\n",
    "\n",
    "        logits = (I_e @ T_e.transpose(-2, -1)) * torch.exp(self.temperature)\n",
    "        labels = torch.arange(logits.shape[0]).to(self.device)\n",
    "        loss_i = nn.functional.cross_entropy(logits.transpose(-2, -1), labels)\n",
    "        loss_t = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu \n",
      "CLIP(\n",
      "  (image_encoder): ImageEncoder(\n",
      "    (linear_project): Conv2d(1, 9, kernel_size=(14, 14), stride=(14, 14))\n",
      "    (positional_embedding): PositionalEmbedding()\n",
      "    (encoder): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoder(\n",
      "        (ln1): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
      "        (mha): MultiHeadAttention(\n",
      "          (W_o): Linear(in_features=9, out_features=9, bias=True)\n",
      "          (heads): ModuleList(\n",
      "            (0-2): 3 x AttentionHead(\n",
      "              (query): Linear(in_features=9, out_features=3, bias=True)\n",
      "              (key): Linear(in_features=9, out_features=3, bias=True)\n",
      "              (value): Linear(in_features=9, out_features=3, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln2): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=9, out_features=36, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=36, out_features=9, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (text_encoder): TextEncoder(\n",
      "    (encoder_embedding): Embedding(256, 32)\n",
      "    (positional_embedding): PositionalEmbedding()\n",
      "    (encoder): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoder(\n",
      "        (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (mha): MultiHeadAttention(\n",
      "          (W_o): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (heads): ModuleList(\n",
      "            (0-7): 8 x AttentionHead(\n",
      "              (query): Linear(in_features=32, out_features=4, bias=True)\n",
      "              (key): Linear(in_features=32, out_features=4, bias=True)\n",
      "              (value): Linear(in_features=32, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 32\n",
    "vit_width = 9\n",
    "img_size = (28, 28)\n",
    "patch_size = (14, 14)\n",
    "n_channels = 1\n",
    "vit_layers = 3\n",
    "vit_heads = 3\n",
    "vocab_size = 256\n",
    "text_width = 32\n",
    "max_seq_length = 32\n",
    "text_heads = 8\n",
    "text_layers = 4\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\n",
    "    \"Using device: \",\n",
    "    device,\n",
    "    f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
    ")\n",
    "\n",
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers,\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader (Fashion MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = load_dataset(\"fashion_mnist\")\n",
    "\n",
    "        self.transform = T.ToTensor()\n",
    "\n",
    "        if train:\n",
    "            self.split = \"train\"\n",
    "        else:\n",
    "            self.split = \"test\"\n",
    "\n",
    "        self.captions = {\n",
    "            0: \"An image of a t-shirt/top\",\n",
    "            1: \"An image of trousers\",\n",
    "            2: \"An image of a pullover\",\n",
    "            3: \"An image of a dress\",\n",
    "            4: \"An image of a coat\",\n",
    "            5: \"An image of a sandal\",\n",
    "            6: \"An image of a shirt\",\n",
    "            7: \"An image of a sneaker\",\n",
    "            8: \"An image of a bag\",\n",
    "            9: \"An image of an ankle boot\",\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows[self.split]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split][i][\"image\"]\n",
    "        img = self.transform(img)\n",
    "\n",
    "        cap, mask = tokenizer(self.captions[self.dataset[self.split][i][\"label\"]])\n",
    "\n",
    "        mask = mask.repeat(len(mask), 1)\n",
    "\n",
    "        return {\"image\": img, \"caption\": cap, \"mask\": mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 32\n",
    "vit_width = 9\n",
    "img_size = (28, 28)\n",
    "patch_size = (14, 14)\n",
    "n_channels = 1\n",
    "vit_layers = 3\n",
    "vit_heads = 3\n",
    "vocab_size = 256\n",
    "text_width = 32\n",
    "max_seq_length = 32\n",
    "text_heads = 8\n",
    "text_layers = 4\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dtunai/Desktop/Work/My Presentations/.venv/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for fashion_mnist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/fashion_mnist\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_set = FashionMNIST(train=True)\n",
    "test_set = FashionMNIST(train=False)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\n",
    "    \"Using device: \",\n",
    "    device,\n",
    "    f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
    ")\n",
    "\n",
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        img, cap, mask = (\n",
    "            data[\"image\"].to(device),\n",
    "            data[\"caption\"].to(device),\n",
    "            data[\"mask\"].to(device),\n",
    "        )\n",
    "        loss = model(img, cap, mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.3f}\")\n",
    "    if loss.item() <= best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), \"model_outputs/clip.pt\")\n",
    "        print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dtunai/Desktop/Work/My Presentations/.venv/lib/python3.10/site-packages/torchvision/transforms.py:36: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 83 %\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"model_outputs/clip.pt\", map_location=device))\n",
    "\n",
    "text = torch.stack([tokenizer(x)[0] for x in test_set.captions.values()]).to(device)\n",
    "mask = torch.stack([tokenizer(x)[1] for x in test_set.captions.values()])\n",
    "mask = (\n",
    "    mask.repeat(1, len(mask[0]))\n",
    "    .reshape(len(mask), len(mask[0]), len(mask[0]))\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data[\"image\"].to(device), data[\"caption\"].to(device)\n",
    "        image_features = model.image_encoder(images)\n",
    "        text_features = model.text_encoder(text, mask=mask)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * (image_features @ text_features.T)).softmax(dim=-1)\n",
    "        _, indices = torch.max(similarity, 1)\n",
    "        pred = torch.stack(\n",
    "            [tokenizer(test_set.captions[int(i)])[0] for i in indices]\n",
    "        ).to(device)\n",
    "        correct += int(sum(torch.sum((pred == labels), dim=1) // len(pred[0])))\n",
    "        total += len(labels)\n",
    "\n",
    "print(f\"\\nModel Accuracy: {100 * correct // total} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-shot Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"model_outputs/clip.pt\", map_location=device))\n",
    "\n",
    "class_names = [\n",
    "    \"t-shirt/top\",\n",
    "    \"trousers\",\n",
    "    \"pullover\",\n",
    "    \"dress\",\n",
    "    \"coat\",\n",
    "    \"sandal\",\n",
    "    \"shirt\",\n",
    "    \"sneaker\",\n",
    "    \"bag\",\n",
    "    \"ankle boot\",\n",
    "]\n",
    "\n",
    "text = torch.stack([tokenizer(x)[0] for x in class_names]).to(device)\n",
    "mask = torch.stack([tokenizer(x)[1] for x in class_names])\n",
    "mask = (\n",
    "    mask.repeat(1, len(mask[0]))\n",
    "    .reshape(len(mask), len(mask[0]), len(mask[0]))\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "idx = 1000\n",
    "\n",
    "img = test_set[idx][\"image\"][None, :]\n",
    "plt.imshow(img[0].permute(1, 2, 0), cmap=\"gray\")\n",
    "plt.title(\n",
    "    tokenizer(test_set[idx][\"caption\"], encode=False, mask=test_set[idx][\"mask\"][0])[0]\n",
    ")\n",
    "plt.show()\n",
    "img = img.to(device)\n",
    "with torch.no_grad():\n",
    "    image_features = model.image_encoder(img)\n",
    "    text_features = model.text_encoder(text, mask=mask)\n",
    "\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{class_names[int(index)]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
